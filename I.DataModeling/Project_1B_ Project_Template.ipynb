{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a list of filepaths to process original event CSV data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check current working directory. Since this might expose local workplace directories, only run this for testing and verification, and be aware of privacy compromising when sharing this notebook to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, get current folder and subfolder for event data, which is stored in `./event_data` directory. Then, loop through all files and get their filepaths for later data wrangling.\n",
    "\n",
    "Notice that, in the below code, I use `glob`, a Python package, for joining filepaths and roots with the subdirectories together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process the files to create a merged data file CSV for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a merged data file CSV must be created, named <font color=red>event_datafile_new.csv</font>, containing data from all the event files whose filepaths were fetched in previous step. The process includes 3 steps:\n",
    "\n",
    "1. Initiate an empty list of rows that will be generated from each file, named `full_data_rows_list`.\n",
    "2. For every filepath in the file path list:\n",
    "    - Read the event data CSV file.\n",
    "    - Extract each data row one by one and append it to the merged data file CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 8056\n"
     ]
    }
   ],
   "source": [
    "full_data_rows_list = [] \n",
    "    \n",
    "for f in file_path_list:\n",
    "\n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    "        for line in csvreader:\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "print(f\"Total number of rows: {len(full_data_rows_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create <font color=red>event_datafile_new.csv</font> that will be used to insert data into the Apache Cassandra tables. For each row fetched from all event data CSV files, extract 11 columns that will be used in later tasks. The list of these 11 columns will be explained next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow([\n",
    "        'artist', 'firstName', 'gender', 'itemInSession',\n",
    "        'lastName', 'length', 'level', 'location',\n",
    "        'sessionId', 'song', 'userId'\n",
    "    ])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((\n",
    "            row[0], row[2], row[3], row[4], \n",
    "            row[5], row[6], row[7], row[8], \n",
    "            row[12], row[13], row[16]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in merged CSV file: 6821\n"
     ]
    }
   ],
   "source": [
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(f\"Total number of rows in merged CSV file: {sum(1 for line in f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Apache Cassandra coding challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the CSV file titled <font color=red>event_datafile_new.csv</font> is completed, located within the Workspace directory. The <font color=red>event_datafile_new.csv</font> contains the following columns: \n",
    "\n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>event_datafile_new.csv</font> after the pre-processing ETL pipeline code in part I is run:<br>\n",
    "\n",
    "<img src=\"./assets/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Apache Cassandra initial configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Create a Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below makes a connection to a Cassandra instance in local machine (127.0.0.1), and establishes connection for executing queries, using a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Create a Keyspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Keyspace is a top-level namespace for Cassandra to define data replication on nodes. The cell below creates a keyspace named `huy_udacity_project1b`, configures replica placement strategy and replication factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyspace created succesfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS huy_udacity_project1b\n",
    "        WITH REPLICATION = {\n",
    "            'class' : 'SimpleStrategy',\n",
    "            'replication_factor' : 1\n",
    "        }\n",
    "    \"\"\")\n",
    "    print(\"Keyspace created succesfully!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Set Keyspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `huy_udacity_project1b` as default keyspace for all queries made in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyspace set successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    session.set_keyspace('huy_udacity_project1b')\n",
    "    print(\"Keyspace set successfully!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project tasks - create queries for the following 3 data questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Question answer approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Question 1 expects Name of the artist, title of the song and length of the track based on `sessionId` and `itemInSession`. As we are working with a NoSQL database, first we need to think about the query and the table required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. The queries\n",
    "\n",
    "As anyone who knows SQL, our query for this is gonna be like:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    artist_name,\n",
    "    song_title,\n",
    "    track_length\n",
    "FROM table_name\n",
    "WHERE\n",
    "    sessionId = 338\n",
    "    AND itemInSession = 4\n",
    "```\n",
    "\n",
    "But first, we need a table. A table that is specifically made for this kind of question and queries it involves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. The table\n",
    "\n",
    "Let's give the table a good, self-explanatory name, like `song_session`.\n",
    "\n",
    "For the table structure:\n",
    "- Columns : all those columns that are either required or involved in query condition, which are:\n",
    "    + Artist name\n",
    "    + Song title\n",
    "    + Song length\n",
    "    + `sessionId`\n",
    "    + `itemInSession`\n",
    "- Primary key : unique identifier of each row. Since we query the data based on `sessionId` and `itemInSession`, let them both be our primary key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Table initiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we first create a table that serves as a response to query 1 above.\n",
    "Before that, as I will rerun this script a lot of time during development, dropping the old one and creating a new one for every rerun is advised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS completed successfully.\n"
     ]
    }
   ],
   "source": [
    "table_query1_name = \"song_session\"\n",
    "\n",
    "query1_dropExist = f\"DROP TABLE IF EXISTS {table_query1_name}\"\n",
    "try:\n",
    "    session.execute(query1_dropExist)\n",
    "    print(\"DROP TABLE IF EXISTS completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be inserted and retrieved in the same order as to how the COMPOSITE PRIMARY KEY is set up. This is important because Apache Cassandra is a partition row store, which means the partition key determines where any particular row is stored and on which node.\n",
    "\n",
    "For this, it is strongly advised to `CREATE` and `INSERT` columns with order as Partition Keys followed by clustering keys followed by other features.\n",
    "\n",
    "Thus, the order of the columns is `sessionId` - `itemInSession` - `artist_name` - `song_title` - `song_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created new table for query 1.\n"
     ]
    }
   ],
   "source": [
    "query1_createNew = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_query1_name} (\n",
    "        sessionId int, \n",
    "        itemInSession int, \n",
    "        artist_name text, \n",
    "        song_title text,\n",
    "        song_length float, \n",
    "        PRIMARY KEY (sessionId, itemInSession)\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.execute(query1_createNew)\n",
    "    print(\"Successfully created new table for query 1.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Insert data into table for the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below fetches data from <font color=red>event_datafile_new.csv</font> file (the merged CSV file we mentioned earlier), based on the table structure we discussed above. \n",
    "\n",
    "Our `INSERT` statement will iterate through each row of the CSV file(line) and Insert the data from the appropriate columns to our table columns. For example, for the `sessionId` the CSV file has the column at index 8, so for the `song_session`'s `sessionId` we will take the value from:\n",
    "- Current row, which is `line`.\n",
    "- `line`'s 9th column which is `line[8] : int(line[8])`. The int here is so that data type matches our table column data-type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully filled song_session with rows.\n"
     ]
    }
   ],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)\n",
    "    for line in csvreader:\n",
    "\n",
    "        query1_insert = f\"\"\"\n",
    "            INSERT INTO {table_query1_name} (\n",
    "                sessionId, \n",
    "                itemInSession, \n",
    "                artist_name, \n",
    "                song_title, \n",
    "                song_length\n",
    "            )\n",
    "        \"\"\"\n",
    "        query1_insert = query1_insert + \"VALUES (%s, %s, %s, %s, %s)\"\n",
    "\n",
    "        try:\n",
    "            session.execute(query1_insert, (\n",
    "                int(line[8]), \n",
    "                int(line[3]), \n",
    "                line[0], \n",
    "                line[9], \n",
    "                float(line[5])\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    print(f\"Successfully filled {table_query1_name} with rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Do a SELECT to verify that the data have been inserted into the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been inserted, we need to verify the insertion with a `SELECT` statement. We are using our question's selection statement based on which we created this table, which I have shown you above right at the section's beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully fetched rows for conditions of query 1 from song_session : \n",
      "\n",
      "Faithless \t Music Matters (Mark Knight Dub) \t 495.30731201171875\n"
     ]
    }
   ],
   "source": [
    "query1_verify = f\"\"\"\n",
    "    SELECT\n",
    "        artist_name, \n",
    "        song_title, \n",
    "        song_length\n",
    "    FROM {table_query1_name}\n",
    "    WHERE sessionId = 338\n",
    "    AND itemInSession = 4\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query1_verify)\n",
    "    print(f\"Sucessfully fetched rows for conditions of query 1 from {table_query1_name} : \\n\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "for row in rows:\n",
    "    print(\n",
    "        row.artist_name, \"\\t\",\n",
    "        row.song_title, \"\\t\",\n",
    "        row.song_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a single record : \n",
    "\n",
    "```\n",
    "Faithless Music Matters (Mark Knight Dub) 495.30731201171875\n",
    "```\n",
    "\n",
    "This means our operation was successful, as it retrieves exactlywhat the question asks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Question answer approach\n",
    "\n",
    "This question expects name of artist, song title and user name, for a specific `userId` and `sessionId` pair of values. This question has 2 additional requirements:\n",
    "- Song names are sorted by `itemInSession`.\n",
    "- Show both first and last names of the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. The queries\n",
    "\n",
    "You will see them in the code cells below in this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. The table\n",
    "\n",
    "Table name : `user_playlist_session`.\n",
    "\n",
    "Table columns, by orders :\n",
    "- `userId` (first primary key)\n",
    "- `sessionId` (second primary key)\n",
    "- `itemInSession` (clustering column, also for sorting the table)\n",
    "- Artist name\n",
    "- Song name\n",
    "- First name\n",
    "- Last name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Table initiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we sort the table by `itemInSession`, we slap it to `CLUSTERING ORDER BY` (I assume we sort by ascend? Since they don't mention which way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS completed successfully.\n",
      "Successfully created new table for query 2.\n"
     ]
    }
   ],
   "source": [
    "table_query2_name = \"user_playlist_session\"\n",
    "\n",
    "query2_dropExist = f\"DROP TABLE IF EXISTS {table_query2_name}\"\n",
    "try:\n",
    "    session.execute(query2_dropExist)\n",
    "    print(\"DROP TABLE IF EXISTS completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "query2_createNew = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_query2_name} (\n",
    "        userId int,\n",
    "        sessionId int,\n",
    "        itemInSession int,\n",
    "        artist_name text, \n",
    "        song_name text,\n",
    "        firstName text, \n",
    "        lastName text,\n",
    "        PRIMARY KEY ((userId, sessionId), itemInSession)\n",
    "    ) WITH CLUSTERING ORDER BY (itemInSession ASC)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.execute(query2_createNew)\n",
    "    print(\"Successfully created new table for query 2.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Insert data into table for the query\n",
    "\n",
    "The cell below fetches data from <font color=red>event_datafile_new.csv</font> file (the merged CSV file we mentioned earlier), based on the table structure we discussed above. Basically the same to previous question, just different columns and their corresponding lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully filled user_playlist_session with rows.\n"
     ]
    }
   ],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)\n",
    "    for line in csvreader:\n",
    "\n",
    "        query2_insert = f\"\"\"\n",
    "            INSERT INTO {table_query2_name} (\n",
    "                userId,\n",
    "                sessionId, \n",
    "                itemInSession,\n",
    "                artist_name, \n",
    "                song_name,\n",
    "                firstName,\n",
    "                lastName\n",
    "            )\n",
    "        \"\"\"\n",
    "        query2_insert = query2_insert + \"VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "        # Based on the order of cols above, it's gonna be 0 - 1 - 3 - 4 - 8 - 9 - 10.\n",
    "        try:\n",
    "            session.execute(query2_insert, (\n",
    "                int(line[10]),\n",
    "                int(line[8]), \n",
    "                int(line[3]),\n",
    "                line[0], \n",
    "                line[9],\n",
    "                line[1],\n",
    "                line[4]\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(f\"Successfully filled {table_query2_name} with rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Do a SELECT to verify that the data have been inserted into the table\n",
    "\n",
    "Once the data has been inserted, we need to verify the insertion with a `SELECT` statement. We are using our question's selection statement based on which we created this table, which I have shown you above right at the section's beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully fetched rows for conditions of query 2 from user_playlist_session : \n",
      "\n",
      "0 \t Down To The Bone \t Keep On Keepin' On \t Sylvie \t Cruz\n",
      "1 \t Three Drives \t Greece 2000 \t Sylvie \t Cruz\n",
      "2 \t Sebastien Tellier \t Kilometer \t Sylvie \t Cruz\n",
      "3 \t Lonnie Gordon \t Catch You Baby (Steve Pitron & Max Sanna Radio Edit) \t Sylvie \t Cruz\n"
     ]
    }
   ],
   "source": [
    "query2_verify = f\"\"\"\n",
    "    SELECT\n",
    "        itemInSession,\n",
    "        artist_name, \n",
    "        song_name, \n",
    "        firstName,\n",
    "        lastName\n",
    "    FROM {table_query2_name}\n",
    "    WHERE userId = 10\n",
    "    AND sessionId = 182\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query2_verify)\n",
    "    print(f\"Sucessfully fetched rows for conditions of query 2 from {table_query2_name} : \\n\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "for row in rows:\n",
    "    print(\n",
    "        row.iteminsession, \"\\t\",\n",
    "        row.artist_name, \"\\t\",\n",
    "        row.song_name, \"\\t\",\n",
    "        row.firstname, \"\\t\",\n",
    "        row.lastname\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the return of `SELECT` query verifies that our code correctly answers the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Question answer approach\n",
    "\n",
    "This question expects first and last names of all the users who has listened to a specific song. Since this question has no hints about which primary key or clustering columns, we must define them ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. The queries\n",
    "\n",
    "You will see them in the code cells below in this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. The table\n",
    "\n",
    "Table name : `song_listeners_session`.\n",
    "\n",
    "Table columns, by orders :\n",
    "- Song's name (first primary key)\n",
    "- `userId` (second primary key)\n",
    "- First name\n",
    "- Last name\n",
    "\n",
    "Although `sessionId` and `userId` can both be used as primary key, as we are querying user's info, `userId` sounds better in terms of data purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Table initiation\n",
    "\n",
    "Similar to other questions, `DROP TABLE IF EXISTS` then `CREATE TABLE IF NOT EXISTS`, with columns in order described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS completed successfully.\n",
      "Successfully created new table for query 3.\n"
     ]
    }
   ],
   "source": [
    "table_query3_name = \"song_listeners_session\"\n",
    "\n",
    "query3_dropExist = f\"DROP TABLE IF EXISTS {table_query3_name}\"\n",
    "try:\n",
    "    session.execute(query3_dropExist)\n",
    "    print(\"DROP TABLE IF EXISTS completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "query3_createNew = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_query3_name} (\n",
    "        song_name text,\n",
    "        userId int,\n",
    "        firstName text, \n",
    "        lastName text,\n",
    "        PRIMARY KEY (song_name, userId)\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.execute(query3_createNew)\n",
    "    print(\"Successfully created new table for query 3.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Insert data into table for the query\n",
    "\n",
    "The cell below fetches data from <font color=red>event_datafile_new.csv</font> file (the merged CSV file we mentioned earlier), based on the table structure we discussed above. Basically the same to previous question, just different columns and their corresponding lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully filled song_listeners_session with rows.\n"
     ]
    }
   ],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader)\n",
    "    for line in csvreader:\n",
    "\n",
    "        query3_insert = f\"\"\"\n",
    "            INSERT INTO {table_query3_name} (\n",
    "                song_name,\n",
    "                userId,\n",
    "                firstName,\n",
    "                lastName\n",
    "            )\n",
    "        \"\"\"\n",
    "        query3_insert = query3_insert + \"VALUES (%s, %s, %s, %s)\"\n",
    "\n",
    "        try:\n",
    "            session.execute(query3_insert, (\n",
    "                line[9],\n",
    "                int(line[10]),\n",
    "                line[1],\n",
    "                line[4]\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(f\"Successfully filled {table_query3_name} with rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Do a SELECT to verify that the data have been inserted into the table\n",
    "\n",
    "Once the data has been inserted, we need to verify the insertion with a `SELECT` statement. We are using our question's selection statement based on which we created this table, which I have shown you above right at the section's beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully fetched rows for conditions of query 3 from song_listeners_session : \n",
      "\n",
      "Jacqueline \t Lynch\n",
      "Tegan \t Levine\n",
      "Sara \t Johnson\n"
     ]
    }
   ],
   "source": [
    "query3_verify = f\"\"\"\n",
    "    SELECT\n",
    "        firstName,\n",
    "        lastName\n",
    "    FROM {table_query3_name}\n",
    "    WHERE song_name = 'All Hands Against His Own'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    rows = session.execute(query3_verify)\n",
    "    print(f\"Sucessfully fetched rows for conditions of query 3 from {table_query3_name} : \\n\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "for row in rows:\n",
    "    print(\n",
    "        row.firstname, \"\\t\",\n",
    "        row.lastname\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the return of `SELECT` query verifies that our code correctly answers the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS all completed successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    session.execute(query1_dropExist)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to drop table {table_query1_name}...\")\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query2_dropExist)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to drop table {table_query2_name}...\")\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    session.execute(query3_dropExist)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to drop table {table_query3_name}...\")\n",
    "    print(e)\n",
    "\n",
    "print(\"DROP TABLE IF EXISTS all completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
